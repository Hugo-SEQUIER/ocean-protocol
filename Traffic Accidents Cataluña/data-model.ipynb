{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hugoz\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import numpy as np\n",
    "import pmdarima as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corriger_encodage(df):\n",
    "    # Fonction pour corriger l'encodage d'une chaîne de caractères\n",
    "    def corriger_chaine(chaine):\n",
    "        if isinstance(chaine, str):\n",
    "            try:\n",
    "                return chaine.encode('latin1').decode('utf-8')\n",
    "            except UnicodeEncodeError:\n",
    "                return chaine\n",
    "        return chaine\n",
    "\n",
    "    # Corriger les valeurs dans le DataFrame\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].apply(corriger_chaine)\n",
    "\n",
    "    # Corriger les noms de colonnes\n",
    "    df.columns = [corriger_chaine(col) for col in df.columns]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = corriger_encodage(pd.read_csv('./data/traffic_to_ml.csv', encoding='unicode_escape').drop(columns=[\"Unnamed: 0\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(numerical_cols) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hugoz\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hugoz\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 128)               6528      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16897 (66.00 KB)\n",
      "Trainable params: 16897 (66.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=num_features, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))  # Output layer for regression\n",
    "\n",
    "# Output layer - adjust according to your problem\n",
    "# For regression: No activation function\n",
    "# For binary classification: 1 neuron, 'sigmoid' activation\n",
    "# For multi-class classification: n neurons (n = number of classes), 'softmax' activation\n",
    "# Compile the model\n",
    "model.compile(optimizer = 'rmsprop',\n",
    "                loss = 'mse',\n",
    "                metrics = ['mae'])\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fatalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data       = df[numerical_cols].sample(frac=1., axis=0)\n",
    "data_train = data.sample(frac=0.7, axis=0)\n",
    "data_test  = data.drop(data_train.index)\n",
    "\n",
    "# ---- Split => x,y (medv is price)\n",
    "#\n",
    "x_train = data_train.drop('Fatalties',  axis=1)\n",
    "y_train = data_train['Fatalties']\n",
    "x_test  = data_test.drop('Fatalties',   axis=1)\n",
    "y_test  = data_test['Fatalties']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = x_train.mean()\n",
    "std  = x_train.std()\n",
    "x_train = (x_train - mean)/std\n",
    "x_test  = (x_test - mean)/std\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_test,  y_test  = np.array(x_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "WARNING:tensorflow:From C:\\Users\\hugoz\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hugoz\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "462/462 [==============================] - 3s 3ms/step - loss: 0.1527 - mae: 0.2177 - val_loss: 0.1810 - val_mae: 0.2471\n",
      "Epoch 2/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1505 - mae: 0.2396 - val_loss: 0.1811 - val_mae: 0.2448\n",
      "Epoch 3/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2397 - val_loss: 0.1811 - val_mae: 0.2431\n",
      "Epoch 4/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2381 - val_loss: 0.1810 - val_mae: 0.2486\n",
      "Epoch 5/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1505 - mae: 0.2401 - val_loss: 0.1811 - val_mae: 0.2454\n",
      "Epoch 6/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1505 - mae: 0.2389 - val_loss: 0.1810 - val_mae: 0.2488\n",
      "Epoch 7/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2399 - val_loss: 0.1811 - val_mae: 0.2453\n",
      "Epoch 8/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1505 - mae: 0.2398 - val_loss: 0.1811 - val_mae: 0.2436\n",
      "Epoch 9/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2389 - val_loss: 0.1811 - val_mae: 0.2429\n",
      "Epoch 10/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1505 - mae: 0.2390 - val_loss: 0.1810 - val_mae: 0.2468\n",
      "Epoch 11/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2393 - val_loss: 0.1810 - val_mae: 0.2499\n",
      "Epoch 12/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1505 - mae: 0.2384 - val_loss: 0.1810 - val_mae: 0.2484\n",
      "Epoch 13/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2390 - val_loss: 0.1810 - val_mae: 0.2528\n",
      "Epoch 14/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2398 - val_loss: 0.1811 - val_mae: 0.2444\n",
      "Epoch 15/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1505 - mae: 0.2390 - val_loss: 0.1811 - val_mae: 0.2435\n",
      "Epoch 16/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2380 - val_loss: 0.1810 - val_mae: 0.2504\n",
      "Epoch 17/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2399 - val_loss: 0.1811 - val_mae: 0.2435\n",
      "Epoch 18/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2392 - val_loss: 0.1810 - val_mae: 0.2464\n",
      "Epoch 19/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1505 - mae: 0.2389 - val_loss: 0.1812 - val_mae: 0.2406\n",
      "Epoch 20/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2398 - val_loss: 0.1812 - val_mae: 0.2402\n",
      "Epoch 21/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1505 - mae: 0.2380 - val_loss: 0.1810 - val_mae: 0.2465\n",
      "Epoch 22/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2402 - val_loss: 0.1811 - val_mae: 0.2420\n",
      "Epoch 23/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2368 - val_loss: 0.1810 - val_mae: 0.2536\n",
      "Epoch 24/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2418 - val_loss: 0.1812 - val_mae: 0.2394\n",
      "Epoch 25/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2380 - val_loss: 0.1810 - val_mae: 0.2464\n",
      "Epoch 26/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1505 - mae: 0.2386 - val_loss: 0.1810 - val_mae: 0.2472\n",
      "Epoch 27/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2405 - val_loss: 0.1812 - val_mae: 0.2397\n",
      "Epoch 28/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1505 - mae: 0.2385 - val_loss: 0.1811 - val_mae: 0.2452\n",
      "Epoch 29/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2395 - val_loss: 0.1812 - val_mae: 0.2408\n",
      "Epoch 30/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1505 - mae: 0.2385 - val_loss: 0.1810 - val_mae: 0.2468\n",
      "Epoch 31/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2408 - val_loss: 0.1812 - val_mae: 0.2404\n",
      "Epoch 32/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1505 - mae: 0.2385 - val_loss: 0.1810 - val_mae: 0.2482\n",
      "Epoch 33/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2397 - val_loss: 0.1810 - val_mae: 0.2473\n",
      "Epoch 34/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1505 - mae: 0.2393 - val_loss: 0.1811 - val_mae: 0.2422\n",
      "Epoch 35/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2380 - val_loss: 0.1811 - val_mae: 0.2437\n",
      "Epoch 36/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1505 - mae: 0.2391 - val_loss: 0.1812 - val_mae: 0.2413\n",
      "Epoch 37/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1505 - mae: 0.2386 - val_loss: 0.1811 - val_mae: 0.2423\n",
      "Epoch 38/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1505 - mae: 0.2383 - val_loss: 0.1810 - val_mae: 0.2467\n",
      "Epoch 39/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2395 - val_loss: 0.1811 - val_mae: 0.2454\n",
      "Epoch 40/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2387 - val_loss: 0.1810 - val_mae: 0.2460\n",
      "Epoch 41/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2386 - val_loss: 0.1810 - val_mae: 0.2479\n",
      "Epoch 42/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2388 - val_loss: 0.1810 - val_mae: 0.2474\n",
      "Epoch 43/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2402 - val_loss: 0.1812 - val_mae: 0.2399\n",
      "Epoch 44/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2382 - val_loss: 0.1811 - val_mae: 0.2451\n",
      "Epoch 45/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2388 - val_loss: 0.1810 - val_mae: 0.2457\n",
      "Epoch 46/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2397 - val_loss: 0.1811 - val_mae: 0.2423\n",
      "Epoch 47/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2379 - val_loss: 0.1810 - val_mae: 0.2495\n",
      "Epoch 48/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2410 - val_loss: 0.1812 - val_mae: 0.2402\n",
      "Epoch 49/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2394 - val_loss: 0.1812 - val_mae: 0.2397\n",
      "Epoch 50/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1505 - mae: 0.2387 - val_loss: 0.1811 - val_mae: 0.2447\n",
      "Epoch 51/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1505 - mae: 0.2400 - val_loss: 0.1811 - val_mae: 0.2421\n",
      "Epoch 52/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1505 - mae: 0.2383 - val_loss: 0.1810 - val_mae: 0.2487\n",
      "Epoch 53/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2380 - val_loss: 0.1810 - val_mae: 0.2496\n",
      "Epoch 54/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1505 - mae: 0.2398 - val_loss: 0.1811 - val_mae: 0.2452\n",
      "Epoch 55/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2381 - val_loss: 0.1811 - val_mae: 0.2447\n",
      "Epoch 56/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.1504 - mae: 0.2389 - val_loss: 0.1811 - val_mae: 0.2430\n",
      "Epoch 57/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1505 - mae: 0.2383 - val_loss: 0.1810 - val_mae: 0.2468\n",
      "Epoch 58/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2383 - val_loss: 0.1810 - val_mae: 0.2511\n",
      "Epoch 59/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2404 - val_loss: 0.1811 - val_mae: 0.2427\n",
      "Epoch 60/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.1504 - mae: 0.2399 - val_loss: 0.1813 - val_mae: 0.2372\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    epochs=60,\n",
    "    validation_data = (x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test / loss      : 0.2177\n",
      "x_test / mae       : 0.2396\n"
     ]
    }
   ],
   "source": [
    "score = history.history[\"mae\"]\n",
    "\n",
    "print('x_test / loss      : {:5.4f}'.format(score[0]))\n",
    "print('x_test / mae       : {:5.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : 0.1258\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction : {:.4f}\".format(predictions[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serious Injuries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data       = df[numerical_cols].sample(frac=1., axis=0)\n",
    "data_train = data.sample(frac=0.7, axis=0)\n",
    "data_test  = data.drop(data_train.index)\n",
    "\n",
    "# ---- Split => x,y (medv is price)\n",
    "#\n",
    "x_train = data_train.drop('Serious Injuries',  axis=1)\n",
    "y_train = data_train['Serious Injuries']\n",
    "x_test  = data_test.drop('Serious Injuries',   axis=1)\n",
    "y_test  = data_test['Serious Injuries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = x_train.mean()\n",
    "std  = x_train.std()\n",
    "x_train = (x_train - mean)/std\n",
    "x_test  = (x_test - mean)/std\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_test,  y_test  = np.array(x_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.7299 - mae: 0.7149 - val_loss: 0.4065 - val_mae: 0.5236\n",
      "Epoch 2/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.3553 - mae: 0.3699 - val_loss: 0.2485 - val_mae: 0.2320\n",
      "Epoch 3/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2969 - mae: 0.2075 - val_loss: 0.2481 - val_mae: 0.1991\n",
      "Epoch 4/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2037 - val_loss: 0.2479 - val_mae: 0.1990\n",
      "Epoch 5/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2038 - val_loss: 0.2479 - val_mae: 0.1982\n",
      "Epoch 6/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2031 - val_loss: 0.2481 - val_mae: 0.1990\n",
      "Epoch 7/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2049 - val_loss: 0.2479 - val_mae: 0.2005\n",
      "Epoch 8/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2967 - mae: 0.2035 - val_loss: 0.2479 - val_mae: 0.1984\n",
      "Epoch 9/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2967 - mae: 0.2032 - val_loss: 0.2482 - val_mae: 0.2015\n",
      "Epoch 10/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2967 - mae: 0.2037 - val_loss: 0.2481 - val_mae: 0.2005\n",
      "Epoch 11/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2965 - mae: 0.2070 - val_loss: 0.2477 - val_mae: 0.2068\n",
      "Epoch 12/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2967 - mae: 0.2045 - val_loss: 0.2483 - val_mae: 0.2029\n",
      "Epoch 13/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2967 - mae: 0.2034 - val_loss: 0.2483 - val_mae: 0.2039\n",
      "Epoch 14/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2966 - mae: 0.2030 - val_loss: 0.2482 - val_mae: 0.2026\n",
      "Epoch 15/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2966 - mae: 0.2053 - val_loss: 0.2479 - val_mae: 0.2002\n",
      "Epoch 16/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2967 - mae: 0.2030 - val_loss: 0.2480 - val_mae: 0.1979\n",
      "Epoch 17/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2966 - mae: 0.2035 - val_loss: 0.2483 - val_mae: 0.2043\n",
      "Epoch 18/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2966 - mae: 0.2040 - val_loss: 0.2481 - val_mae: 0.1998\n",
      "Epoch 19/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2966 - mae: 0.2030 - val_loss: 0.2478 - val_mae: 0.2019\n",
      "Epoch 20/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2967 - mae: 0.2032 - val_loss: 0.2479 - val_mae: 0.2006\n",
      "Epoch 21/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2967 - mae: 0.2026 - val_loss: 0.2481 - val_mae: 0.1987\n",
      "Epoch 22/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2966 - mae: 0.2025 - val_loss: 0.2486 - val_mae: 0.2083\n",
      "Epoch 23/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2966 - mae: 0.2058 - val_loss: 0.2481 - val_mae: 0.1999\n",
      "Epoch 24/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2056 - val_loss: 0.2479 - val_mae: 0.2008\n",
      "Epoch 25/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2052 - val_loss: 0.2478 - val_mae: 0.2044\n",
      "Epoch 26/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2966 - mae: 0.2045 - val_loss: 0.2480 - val_mae: 0.1974\n",
      "Epoch 27/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2966 - mae: 0.2050 - val_loss: 0.2479 - val_mae: 0.1980\n",
      "Epoch 28/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2967 - mae: 0.2037 - val_loss: 0.2484 - val_mae: 0.2049\n",
      "Epoch 29/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2966 - mae: 0.2045 - val_loss: 0.2482 - val_mae: 0.2021\n",
      "Epoch 30/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2967 - mae: 0.2025 - val_loss: 0.2484 - val_mae: 0.2047\n",
      "Epoch 31/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2966 - mae: 0.2046 - val_loss: 0.2479 - val_mae: 0.1983\n",
      "Epoch 32/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2039 - val_loss: 0.2478 - val_mae: 0.2025\n",
      "Epoch 33/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2043 - val_loss: 0.2486 - val_mae: 0.2089\n",
      "Epoch 34/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2053 - val_loss: 0.2482 - val_mae: 0.2016\n",
      "Epoch 35/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2036 - val_loss: 0.2481 - val_mae: 0.1988\n",
      "Epoch 36/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2966 - mae: 0.2036 - val_loss: 0.2479 - val_mae: 0.1982\n",
      "Epoch 37/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2967 - mae: 0.2030 - val_loss: 0.2481 - val_mae: 0.2001\n",
      "Epoch 38/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2966 - mae: 0.2045 - val_loss: 0.2479 - val_mae: 0.2000\n",
      "Epoch 39/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2967 - mae: 0.2025 - val_loss: 0.2482 - val_mae: 0.2011\n",
      "Epoch 40/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2967 - mae: 0.2028 - val_loss: 0.2482 - val_mae: 0.2019\n",
      "Epoch 41/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2967 - mae: 0.2036 - val_loss: 0.2481 - val_mae: 0.1997\n",
      "Epoch 42/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2060 - val_loss: 0.2479 - val_mae: 0.2004\n",
      "Epoch 43/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2052 - val_loss: 0.2478 - val_mae: 0.2013\n",
      "Epoch 44/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2036 - val_loss: 0.2482 - val_mae: 0.2026\n",
      "Epoch 45/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2967 - mae: 0.2027 - val_loss: 0.2484 - val_mae: 0.2052\n",
      "Epoch 46/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2967 - mae: 0.2040 - val_loss: 0.2482 - val_mae: 0.2012\n",
      "Epoch 47/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2046 - val_loss: 0.2483 - val_mae: 0.2036\n",
      "Epoch 48/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2966 - mae: 0.2040 - val_loss: 0.2480 - val_mae: 0.1982\n",
      "Epoch 49/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2042 - val_loss: 0.2479 - val_mae: 0.2003\n",
      "Epoch 50/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2966 - mae: 0.2034 - val_loss: 0.2482 - val_mae: 0.2024\n",
      "Epoch 51/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2966 - mae: 0.2043 - val_loss: 0.2482 - val_mae: 0.2024\n",
      "Epoch 52/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2043 - val_loss: 0.2480 - val_mae: 0.1978\n",
      "Epoch 53/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2967 - mae: 0.2026 - val_loss: 0.2483 - val_mae: 0.2031\n",
      "Epoch 54/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2966 - mae: 0.2046 - val_loss: 0.2478 - val_mae: 0.2020\n",
      "Epoch 55/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2967 - mae: 0.2027 - val_loss: 0.2480 - val_mae: 0.1985\n",
      "Epoch 56/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2029 - val_loss: 0.2480 - val_mae: 0.1975\n",
      "Epoch 57/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2967 - mae: 0.2033 - val_loss: 0.2479 - val_mae: 0.2003\n",
      "Epoch 58/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2967 - mae: 0.2029 - val_loss: 0.2481 - val_mae: 0.2004\n",
      "Epoch 59/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2048 - val_loss: 0.2479 - val_mae: 0.1981\n",
      "Epoch 60/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2966 - mae: 0.2032 - val_loss: 0.2479 - val_mae: 0.2007\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    epochs=60,\n",
    "    validation_data = (x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test / loss      : 0.7149\n",
      "x_test / mae       : 0.3699\n"
     ]
    }
   ],
   "source": [
    "score = history.history[\"mae\"]\n",
    "\n",
    "print('x_test / loss      : {:5.4f}'.format(score[0]))\n",
    "print('x_test / mae       : {:5.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : 0.9954\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction : {:.4f}\".format(predictions[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Victims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data       = df[numerical_cols].sample(frac=1., axis=0)\n",
    "data_train = data.sample(frac=0.7, axis=0)\n",
    "data_test  = data.drop(data_train.index)\n",
    "\n",
    "# ---- Split => x,y (medv is price)\n",
    "#\n",
    "x_train = data_train.drop('Serious Injuries',  axis=1)\n",
    "y_train = data_train['Serious Injuries']\n",
    "x_test  = data_test.drop('Serious Injuries',   axis=1)\n",
    "y_test  = data_test['Serious Injuries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = x_train.mean()\n",
    "std  = x_train.std()\n",
    "x_train = (x_train - mean)/std\n",
    "x_test  = (x_test - mean)/std\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "x_test,  y_test  = np.array(x_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2013 - val_loss: 0.3363 - val_mae: 0.2036\n",
      "Epoch 2/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2588 - mae: 0.2006 - val_loss: 0.3363 - val_mae: 0.2053\n",
      "Epoch 3/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2015 - val_loss: 0.3363 - val_mae: 0.2051\n",
      "Epoch 4/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2024 - val_loss: 0.3363 - val_mae: 0.2090\n",
      "Epoch 5/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2005 - val_loss: 0.3363 - val_mae: 0.2084\n",
      "Epoch 6/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2019 - val_loss: 0.3363 - val_mae: 0.2042\n",
      "Epoch 7/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2011 - val_loss: 0.3363 - val_mae: 0.2045\n",
      "Epoch 8/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2034 - val_loss: 0.3364 - val_mae: 0.2087\n",
      "Epoch 9/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2023 - val_loss: 0.3363 - val_mae: 0.2032\n",
      "Epoch 10/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2010 - val_loss: 0.3363 - val_mae: 0.2061\n",
      "Epoch 11/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2011 - val_loss: 0.3364 - val_mae: 0.2135\n",
      "Epoch 12/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2015 - val_loss: 0.3363 - val_mae: 0.2085\n",
      "Epoch 13/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2026 - val_loss: 0.3363 - val_mae: 0.2094\n",
      "Epoch 14/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2021 - val_loss: 0.3363 - val_mae: 0.2040\n",
      "Epoch 15/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2020 - val_loss: 0.3363 - val_mae: 0.2030\n",
      "Epoch 16/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2010 - val_loss: 0.3363 - val_mae: 0.2042\n",
      "Epoch 17/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2007 - val_loss: 0.3363 - val_mae: 0.2067\n",
      "Epoch 18/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2017 - val_loss: 0.3363 - val_mae: 0.2049\n",
      "Epoch 19/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2588 - mae: 0.2012 - val_loss: 0.3363 - val_mae: 0.2055\n",
      "Epoch 20/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2016 - val_loss: 0.3363 - val_mae: 0.2033\n",
      "Epoch 21/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2012 - val_loss: 0.3363 - val_mae: 0.2053\n",
      "Epoch 22/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2009 - val_loss: 0.3363 - val_mae: 0.2062\n",
      "Epoch 23/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2014 - val_loss: 0.3363 - val_mae: 0.2028\n",
      "Epoch 24/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2023 - val_loss: 0.3363 - val_mae: 0.2025\n",
      "Epoch 25/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2588 - mae: 0.2016 - val_loss: 0.3363 - val_mae: 0.2062\n",
      "Epoch 26/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2588 - mae: 0.2013 - val_loss: 0.3363 - val_mae: 0.2025\n",
      "Epoch 27/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2014 - val_loss: 0.3363 - val_mae: 0.2038\n",
      "Epoch 28/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2040 - val_loss: 0.3363 - val_mae: 0.2083\n",
      "Epoch 29/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2588 - mae: 0.2026 - val_loss: 0.3363 - val_mae: 0.2054\n",
      "Epoch 30/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2588 - mae: 0.2015 - val_loss: 0.3364 - val_mae: 0.2108\n",
      "Epoch 31/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2588 - mae: 0.2015 - val_loss: 0.3365 - val_mae: 0.2162\n",
      "Epoch 32/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2033 - val_loss: 0.3363 - val_mae: 0.2051\n",
      "Epoch 33/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2008 - val_loss: 0.3363 - val_mae: 0.2035\n",
      "Epoch 34/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2588 - mae: 0.2014 - val_loss: 0.3363 - val_mae: 0.2037\n",
      "Epoch 35/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2588 - mae: 0.2020 - val_loss: 0.3363 - val_mae: 0.2090\n",
      "Epoch 36/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2588 - mae: 0.2015 - val_loss: 0.3363 - val_mae: 0.2078\n",
      "Epoch 37/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2013 - val_loss: 0.3363 - val_mae: 0.2039\n",
      "Epoch 38/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2588 - mae: 0.2009 - val_loss: 0.3363 - val_mae: 0.2077\n",
      "Epoch 39/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2036 - val_loss: 0.3363 - val_mae: 0.2047\n",
      "Epoch 40/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2588 - mae: 0.2013 - val_loss: 0.3363 - val_mae: 0.2100\n",
      "Epoch 41/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2020 - val_loss: 0.3363 - val_mae: 0.2026\n",
      "Epoch 42/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2012 - val_loss: 0.3363 - val_mae: 0.2037\n",
      "Epoch 43/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2018 - val_loss: 0.3363 - val_mae: 0.2074\n",
      "Epoch 44/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2022 - val_loss: 0.3363 - val_mae: 0.2095\n",
      "Epoch 45/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2034 - val_loss: 0.3364 - val_mae: 0.2108\n",
      "Epoch 46/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2012 - val_loss: 0.3363 - val_mae: 0.2064\n",
      "Epoch 47/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2015 - val_loss: 0.3364 - val_mae: 0.2103\n",
      "Epoch 48/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2025 - val_loss: 0.3363 - val_mae: 0.2057\n",
      "Epoch 49/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2588 - mae: 0.2012 - val_loss: 0.3363 - val_mae: 0.2030\n",
      "Epoch 50/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2588 - mae: 0.2001 - val_loss: 0.3363 - val_mae: 0.2087\n",
      "Epoch 51/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2588 - mae: 0.2024 - val_loss: 0.3363 - val_mae: 0.2047\n",
      "Epoch 52/60\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 0.2588 - mae: 0.2023 - val_loss: 0.3363 - val_mae: 0.2077\n",
      "Epoch 53/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2587 - mae: 0.2031 - val_loss: 0.3365 - val_mae: 0.2129\n",
      "Epoch 54/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2018 - val_loss: 0.3363 - val_mae: 0.2064\n",
      "Epoch 55/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2030 - val_loss: 0.3364 - val_mae: 0.2104\n",
      "Epoch 56/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2029 - val_loss: 0.3364 - val_mae: 0.2089\n",
      "Epoch 57/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2015 - val_loss: 0.3363 - val_mae: 0.2048\n",
      "Epoch 58/60\n",
      "462/462 [==============================] - 2s 3ms/step - loss: 0.2588 - mae: 0.2013 - val_loss: 0.3363 - val_mae: 0.2098\n",
      "Epoch 59/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2028 - val_loss: 0.3364 - val_mae: 0.2120\n",
      "Epoch 60/60\n",
      "462/462 [==============================] - 1s 3ms/step - loss: 0.2588 - mae: 0.2023 - val_loss: 0.3364 - val_mae: 0.2103\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    epochs=60,\n",
    "    validation_data = (x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test / loss      : 0.2013\n",
      "x_test / mae       : 0.2006\n"
     ]
    }
   ],
   "source": [
    "score = history.history[\"mae\"]\n",
    "\n",
    "print('x_test / loss      : {:5.4f}'.format(score[0]))\n",
    "print('x_test / mae       : {:5.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction : 0.9900\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction : {:.4f}\".format(predictions[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])  # Ensure 'Date' is a datetime object\n",
    "df.set_index('Date', inplace=True)  # Set 'Date' as the index\n",
    "\n",
    "monthly_accidents = df.resample('M').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing stepwise search to minimize aic\n",
      " ARIMA(0,1,0)(0,0,0)[0] intercept   : AIC=1320.489, Time=0.06 sec\n",
      " ARIMA(1,1,0)(0,0,0)[0] intercept   : AIC=1308.285, Time=0.05 sec\n",
      " ARIMA(0,1,1)(0,0,0)[0] intercept   : AIC=1300.348, Time=0.05 sec\n",
      " ARIMA(0,1,0)(0,0,0)[0]             : AIC=1318.505, Time=0.02 sec\n",
      " ARIMA(1,1,1)(0,0,0)[0] intercept   : AIC=inf, Time=0.19 sec\n",
      " ARIMA(0,1,2)(0,0,0)[0] intercept   : AIC=1289.273, Time=0.09 sec\n",
      " ARIMA(1,1,2)(0,0,0)[0] intercept   : AIC=inf, Time=0.24 sec\n",
      " ARIMA(0,1,3)(0,0,0)[0] intercept   : AIC=1285.273, Time=0.14 sec\n",
      " ARIMA(1,1,3)(0,0,0)[0] intercept   : AIC=inf, Time=0.32 sec\n",
      " ARIMA(0,1,4)(0,0,0)[0] intercept   : AIC=inf, Time=0.26 sec\n",
      " ARIMA(1,1,4)(0,0,0)[0] intercept   : AIC=inf, Time=0.36 sec\n",
      " ARIMA(0,1,3)(0,0,0)[0]             : AIC=1286.173, Time=0.07 sec\n",
      "\n",
      "Best model:  ARIMA(0,1,3)(0,0,0)[0] intercept\n",
      "Total fit time: 1.831 seconds\n"
     ]
    }
   ],
   "source": [
    "train = monthly_accidents[:'2018-01-01']  # Replace with your split date\n",
    "test = monthly_accidents['2018-01-01':]  # Replace with your split date\n",
    "\n",
    "# Build and fit the ARIMA model\n",
    "model = pm.auto_arima(monthly_accidents, seasonal=True, m=1,\n",
    "                      start_p=0, start_q=0, \n",
    "                      max_p=10, max_q=10, \n",
    "                      start_P=0, start_Q=0, \n",
    "                      max_P=10, max_Q=10, \n",
    "                      d=None, D=None, \n",
    "                      trace=True,\n",
    "                      error_action='ignore',  \n",
    "                      suppress_warnings=True, \n",
    "                      stepwise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  144\n",
      "Model:               SARIMAX(0, 1, 3)   Log Likelihood                -637.637\n",
      "Date:                Thu, 25 Jan 2024   AIC                           1285.273\n",
      "Time:                        12:29:11   BIC                           1300.088\n",
      "Sample:                    01-31-2010   HQIC                          1291.293\n",
      "                         - 12-31-2021                                         \n",
      "Covariance Type:                  opg                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept     -0.3423      0.135     -2.537      0.011      -0.607      -0.078\n",
      "ma.L1         -0.5489      0.064     -8.571      0.000      -0.674      -0.423\n",
      "ma.L2         -0.1981      0.093     -2.133      0.033      -0.380      -0.016\n",
      "ma.L3         -0.1979      0.087     -2.274      0.023      -0.368      -0.027\n",
      "sigma2       431.0424     44.108      9.772      0.000     344.592     517.493\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):                29.34\n",
      "Prob(Q):                              0.96   Prob(JB):                         0.00\n",
      "Heteroskedasticity (H):               1.15   Skew:                            -0.65\n",
      "Prob(H) (two-sided):                  0.63   Kurtosis:                         4.80\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022-01-31    111.831703\n",
       "2022-02-28    117.149975\n",
       "2022-03-31    117.420747\n",
       "2022-04-30    117.078425\n",
       "2022-05-31    116.736103\n",
       "2022-06-30    116.393780\n",
       "2022-07-31    116.051458\n",
       "2022-08-31    115.709135\n",
       "2022-09-30    115.366813\n",
       "2022-10-31    115.024490\n",
       "Freq: M, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
